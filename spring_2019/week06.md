This week was further research on the word2vec model.
Still implementing the skip-gram model due to blockage in tensorflow.
More specifically, tensorflow has a graph visualization/debugging tool called
tensorboard. Variables can be stored in different name spaces thus creating a more easily understandable visualization of the neural network.
https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf

Above is the research done by Mikolov and fellow Google researchers on original model of word2vec.
http://web.stanford.edu/class/cs20si/syllabus.html
I am following a stanford course on how to use tensorflow. 
No surprise because the stanford campus is right next to Google Research.

By Next friday a naive implementation without tensor board should be done.
